#1.2.7.12已加入下面功能
# 0. 自己從0開始練又很難直接成功，只好先載個預訓練壓壓驚，但是好像成功了，但還是不管先放著導入模組
# 1. 寫另外一個.py利用 timm 載入 google/vit-base-patch16-224-in21k 作為 encoder 導入model.py
# 2. 不要更動原有的模型架構，方便日後的改動
# 3. 支持不同尺寸（例如 400x400）的輸入，並對位置嵌入做插值。


# 不知道為什麼，加上validation的功能之後，模型訓練的效率就變很差
# 模型結構大改
# 改到後來從 1.2再出發
# 後來增加了在10次訓練後產生驗證圖檔
# 如果驗證比較差，就會在gradient decent 的時候返回之前比較好的點
# 細節上真的還是不太行
# 後來改成
# 1. 使用預訓練 ViT ；
# 2. 加入 UNet‐style skip connections 以及 
# 3. 調整 patch 方案（使用重疊 patch 與多尺度概念）

#結果變得很棒，直接用FB 和 google 的預處理模型好像比較威
# https://arxiv.org/abs/2203.16527
# https://arxiv.org/abs/2010.11929

# losses.py改成先lambda_dice_bce, lambda_focal 穩定到一定的程度後，開始最佳化lambda_tversky, lambda_boundary
# 會先載入預訓練的模型，如果有的話

#In losses.py:
#Added clamp() to prevent extreme values in sigmoid outputs
#Protected against division by zero in all loss calculations
#Added NaN/Inf detection in the combined loss and fallback mechanism
#Limited the influence of distance maps in boundary loss
#Improved numerical stability in focal loss by constraining the range of exponents

#In train.py:
#Added gradient clipping to prevent exploding gradients
#Added learning rate scheduler to automatically reduce learning rate when validation loss plateaus
#Added batch skipping when NaN losses are detected
#Improved error handling throughout the training loop
#Added early stopping mechanism to prevent overfitting
#Tracked number of valid batches to correctly calculate average loss

# losses.py太複雜，又改回簡單的Dice+BCE



##系統的架構

party-model/
├── configs/              # Configuration templates
│   ├── default.yaml
├── data/                # Dataset storage
│   ├── pretrained model/
│        ├── pretrained_model.pth
│   ├── train/
│        ├── images/
│        ├── masks/
│   ├── test/
│        ├── images/
│        ├── masks/
│   ├── val/
│        ├── images/
│        ├── masks/
├── outputs/             # Training artifacts
│   ├── models/          # Checkpoints
│   ├── predictions/     # Inference results
│   └── tensorboard/     # Training metrics
├── src/                 # Core implementation
│   ├── dataset.py        # 數據讀取與增強
│   ├── model.py          # 原始 TransUNet 模型
│   ├── model_with_timm.py# 基於 timm 的 ViT encoder 模型
│   ├── losses.py         # 損失函數實現
│   ├── train.py          # 模型訓練流程
│   ├── predict.py        # 預測與推理模塊
│   ├── utils.py          # 輔助工具（例如可視化）
│   └── postprocess.py    # 分割結果後處理模塊
└── requirements.txt     # Dependency list
└── README,md

##模型的架構
         ┌─────────────────────────────────────────┐
         │              Input Image                │
         │             (e.g., 400×400)             │
         └─────────────────────────────────────────┘
                          │
                          ▼
         ┌─────────────────────────────────────────┐
         │     TimMEncoderWrapper (ViT Encoder)    │
         │  - 利用 timm 預訓練 ViT 模型             │
         │  - 將影像分割成 patches 送入 Transformer │
         │  - 插值位置嵌入支援不同尺寸               │
         └─────────────────────────────────────────┘
                          │
             ┌────────────┴─────────────┐
             │                          │
  ┌─────────────────────────────┐     ┌───────────────────────┐
  │  最終 Tokens (N)            │     │   Skip Tokens (N)     │
  │ (最後一層，移除 class token) │     │ (來源自中間層, 如第6層) │
  └─────────────────────────────┘     └───────────────────────┘
             │                          │
             └────────────┬─────────────┘
                          │
                 轉換為 2D Feature Maps   
         (Reshape token序列為 [B, 768, H, W])
                          │
                          ▼
         ┌───────────────────────────────────────────┐
         │  Concatenation of Skip & Final Features   │
         │          (Channel: 768+768 = 1536)        │
         └───────────────────────────────────────────┘
                          │
                          ▼
         ┌───────────────────────────────────────────┐
         │        UNet-style Decoder                 │
         │  - 多層 Conv2d + BN + ReLU + Upsampling    │
         │  - 將 encoder 特徵逐層上採樣，恢復空間解析度 │
         └───────────────────────────────────────────┘
                          │
                          ▼
         ┌───────────────────────────────────────────┐
         │        Output Segmentation Map            │
         │                (1 channel)                │
         └───────────────────────────────────────────┘
#Encoder (TimMEncoderWrapper)：
#利用 timm 提供的 vit_base_patch16_224_in21k 模型，先將輸入影像經過 patch_embedding、加入 class token 與位置嵌入後，送入 Transformer blocks 進行全局特徵抽取。透過插值方法，使得位置嵌入能夠適應不同尺寸的影像。

#Skip Connection：
#除了取 encoder 最終輸出（去除 class token）之外，從中間某一層（例如第6層）取出 skip token，進行 reshape（轉換成 2D feature map），與最終特徵一起串聯。

#Decoder (UNet-style)：
#經過串聯後的特徵圖進入 decoder，採用多層卷積、Batch Normalization、ReLU 激勵以及上採樣操作，逐步恢復空間尺寸，最終輸出單通道的分割結果。

#後處理（POSSPROSSES）：
#可透過形態學操作、CRF 或多尺度融合等方式進一步改善預測細節，對邊界精度和雜訊進行修正。


# ==== configs/default.yaml ====
data_path: "data/"
batch_size: 4
epochs: 500
lr: 1.0e-5         # 使用科學計數法表示
patch_size: 400    # 可調整，值越小識別越精細 (理論上是這樣，但實際上要看辨別的標的)
val_split: 0.2     # 小數形式


# 模型架構選項：
#  - "TransUNet": 使用原始 TransUNet 模型
#  - "TransUNetWithTimm": 使用 timm 載入的 ViT encoder，並保持原有 decoder 結構
model_type: "TransUNet"

num_decoder_conv_layers: 120   # 可隨意修改 decoder 中卷積層數量（例如預設 30 層），根據過往的研究，層數太高會丟失細節，太低會分辨不佳，大概20-30層之間
#25層好像也不太可以，目前的測試是80-100層，對篩管好像不錯。


# ==== src/dataset.py ====
import os
import cv2
import numpy as np
import albumentations as A
from torch.utils.data import Dataset
import torch

class SegmentationDataset(Dataset):
    def __init__(self, data_root, mode='train', patch_size=400):
        self.image_dir = os.path.join(data_root, mode, 'images')
        self.mask_dir = os.path.join(data_root, mode, 'masks')
        self.patch_size = patch_size
        self.mode = mode
        
        # 加載文件列表並驗證
        self.image_files = sorted([f for f in os.listdir(self.image_dir) if f.endswith('.jpg')])
        self.mask_files = sorted([f for f in os.listdir(self.mask_dir) if f.endswith('.png')])
        assert len(self.image_files) == len(self.mask_files), "image和mask數量不匹配"
        
        # 數據增強配置，這裡使用了各種小技巧，讓模型學一些麻煩奇怪的形狀
        self.transform = A.Compose([
        
            ##隨機將圖像旋轉90度的倍數（即90、180、270或360度）。
            #這是一個離散的旋轉操作，保證旋轉後的圖像依舊不失真，因為90度旋轉通常不會引入插值問題。
            #對於細胞圖像來說，讓模型在不同角度下都能識別相同的細胞結構，提高模型的旋轉不變性。
            A.RandomRotate90(p=0.5),
            
            ##HorizontalFlip (水平翻轉)-將圖像沿垂直軸對調，相當於左右鏡像。
            #這個操作模擬了左右對稱的變化，有助於模型學習在水平方向上不變的特徵。
            #對於細胞圖像，這有助於減少因細胞分布方向帶來的偏差，讓模型對左右方向的細節均能良好識別。
            A.HorizontalFlip(p=0.5),
            
            ##VerticalFlip (垂直翻轉)類似水平翻轉
            A.VerticalFlip(p=0.5),
            
            ##RandomBrightnessContrast-隨機調整圖像的亮度和對比度。
            #通常會在一個預設範圍內隨機增加或減少亮度，使得圖像看起來更亮或更暗；同時對比度也會隨之變化。
            #這個操作模擬了不同照明條件下的拍攝效果，讓模型在面對不同曝光、亮度或對比度情況時，都能穩定識別細胞的邊緣和結構。
            A.RandomBrightnessContrast(p=0.2),
            
            ##彈性變換 (Elastic Transformation)-將圖片進行非剛性的、局部的隨機變形。它模擬了物體的彈性變形，就像真實世界中柔軟物體可能會發生的扭曲與拉伸。
            #通常會先生成一個隨機的位移場（即每個像素都有一個位移向量），然後利用平滑（例如高斯模糊）過程使位移場更加連續和平滑。
            #這個變換可以用參數如 alpha（控制變形強度）、sigma（控制平滑程度）以及 alpha_affine（控制仿射變換的程度）來調整。
            #這種變換對於細胞圖像來說，能模擬細胞在不同環境下的微小變形，同時保留邊緣結構，讓模型學習更靈活的表達。
            A.ElasticTransform(alpha=1, sigma=50, p=0.2),#alpha_affine=50移除，太新了
            
            ##網格扭曲 (Grid Distortion)-將圖像分割成一個格子網，每個格子內的像素通過縮放或扭曲變換，從而在局部產生變形。
            #將圖像劃分成固定數目的小格子（由參數如 num_steps 決定），然後在每個格子的邊緣進行隨機擾動。
            #通過 distort_limit 等參數設置每個格子可以變形的幅度，確保變形幅度適中，不會破壞全局結構。
            #此方法主要用於生成局部微調的變化，幫助模型在遇到真實圖像中出現的局部形狀變化時，依然能準確分辨細節，例如細胞間隙的位置和連續性。
            A.GridDistortion(num_steps=5, distort_limit=0.3, p=0.2)
        ], additional_targets={'mask': 'mask'})
        
        # 預先生成所有 patches
        self.patches = []
        self._precompute_patches()

    def _precompute_patches(self):
        for img_name, mask_name in zip(self.image_files, self.mask_files):
            image = cv2.cvtColor(cv2.imread(os.path.join(self.image_dir, img_name)), cv2.COLOR_BGR2RGB)
            mask = cv2.imread(os.path.join(self.mask_dir, mask_name), cv2.IMREAD_GRAYSCALE)
            
            # 驗證尺寸
            assert image.shape[:2] == mask.shape, f"{img_name} 尺寸不匹配"
            
            # 生成patches（此處 stride 改為 patch_size//2 ）
            patches = self._extract_patches(image, mask)
            self.patches.extend(patches)

    def _extract_patches(self, image, mask):
        h, w = image.shape[:2]
        stride = self.patch_size // 2   # 使用重疊 patch
        patches = []
        
        for y in range(0, h - self.patch_size + 1, stride):
            for x in range(0, w - self.patch_size + 1, stride):
                img_patch = image[y:y+self.patch_size, x:x+self.patch_size]
                mask_patch = mask[y:y+self.patch_size, x:x+self.patch_size]
                
                # 訓練時應用數據增強
                if self.mode == 'train':
                    augmented = self.transform(image=img_patch, mask=mask_patch)
                    img_patch = augmented['image']
                    mask_patch = augmented['mask']
                
                patches.append((img_patch, mask_patch))
        return patches

    def __len__(self):
        return len(self.patches)

    def __getitem__(self, idx):
        img_patch, mask_patch = self.patches[idx]
        
        # 歸一化並轉換為 Tensor
        img_tensor = torch.from_numpy(img_patch.astype(np.float32) / 255.0).permute(2, 0, 1)
        mask_tensor = torch.from_numpy((mask_patch / 255.0).astype(np.float32)).unsqueeze(0)
        
        return img_tensor, mask_tensor


# ==== src/model.py ====
import math
import torch
import torch.nn as nn
import torch.nn.functional as F

#########################################
# DropPath (Stochastic Depth) 
#########################################
class DropPath(nn.Module):
    """Drop paths (Stochastic Depth) per sample when applied in main path of residual blocks."""
    def __init__(self, drop_prob: float = 0.):
        super().__init__()
        self.drop_prob = drop_prob

    def forward(self, x):
        return drop_path(x, self.drop_prob, self.training)

def drop_path(x, drop_prob: float = 0., training: bool = False):
    if drop_prob == 0. or not training:
        return x
    keep_prob = 1 - drop_prob
    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # 為每個 sample 生成遮罩
    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)
    random_tensor.floor_()
    return x.div(keep_prob) * random_tensor

#########################################
# Patch Embedding
#########################################
class PatchEmbed(nn.Module):
    """
    將圖像分割成 patch 並以卷積映射到 embed_dim
    """
    def __init__(self, img_size=400, patch_size=16, in_chans=3, embed_dim=768):
        super().__init__()
        self.img_size = img_size
        self.patch_size = patch_size
        self.grid_size = (img_size // patch_size, img_size // patch_size)
        self.num_patches = self.grid_size[0] * self.grid_size[1]
        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)
    
    def forward(self, x):
        # x: [B, C, H, W]
        x = self.proj(x)  # [B, embed_dim, H/patch_size, W/patch_size]
        x = x.flatten(2).transpose(1, 2)  # [B, num_patches, embed_dim]
        return x

#########################################
# MLP block
#########################################
class MLP(nn.Module):
    def __init__(self, in_features, hidden_features=None, dropout=0.0):
        super().__init__()
        hidden_features = hidden_features if hidden_features is not None else in_features
        self.fc1 = nn.Linear(in_features, hidden_features)
        self.act = nn.GELU()
        self.fc2 = nn.Linear(hidden_features, in_features)
        self.drop = nn.Dropout(dropout)
    
    def forward(self, x):
        x = self.fc1(x)
        x = self.act(x)
        x = self.drop(x)
        x = self.fc2(x)
        x = self.drop(x)
        return x

#########################################
# Multi-head Self-Attention
#########################################
class Attention(nn.Module):
    def __init__(self, dim, num_heads=12, dropout=0.0):
        super().__init__()
        self.num_heads = num_heads
        head_dim = dim // num_heads
        self.scale = head_dim ** -0.5

        self.qkv = nn.Linear(dim, dim * 3, bias=True)
        self.attn_drop = nn.Dropout(dropout)
        self.proj = nn.Linear(dim, dim)
        self.proj_drop = nn.Dropout(dropout)
    
    def forward(self, x):
        # x: [B, N, dim]
        B, N, C = x.shape
        qkv = self.qkv(x)  # [B, N, 3*dim]
        qkv = qkv.reshape(B, N, 3, self.num_heads, C // self.num_heads)
        qkv = qkv.permute(2, 0, 3, 1, 4)  # [3, B, num_heads, N, head_dim]
        q, k, v = qkv[0], qkv[1], qkv[2]   # each: [B, num_heads, N, head_dim]
        attn = (q @ k.transpose(-2, -1)) * self.scale  # [B, num_heads, N, N]
        attn = attn.softmax(dim=-1)
        attn = self.attn_drop(attn)
        x = (attn @ v)  # [B, num_heads, N, head_dim]
        x = x.transpose(1, 2).reshape(B, N, C)  # [B, N, dim]
        x = self.proj(x)
        x = self.proj_drop(x)
        return x

#########################################
# Transformer Block
#########################################
class TransformerBlock(nn.Module):
    def __init__(self, dim, num_heads=12, mlp_ratio=4.0, dropout=0.0, drop_path=0.0):
        super().__init__()
        self.norm1 = nn.LayerNorm(dim)
        self.attn = Attention(dim, num_heads=num_heads, dropout=dropout)
        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
        self.norm2 = nn.LayerNorm(dim)
        mlp_hidden_dim = int(dim * mlp_ratio)
        self.mlp = MLP(dim, hidden_features=mlp_hidden_dim, dropout=dropout)
    
    def forward(self, x):
        x = x + self.drop_path(self.attn(self.norm1(x)))
        x = x + self.drop_path(self.mlp(self.norm2(x)))
        return x

#########################################
# Full Vision Transformer
#########################################
class VisionTransformer(nn.Module):
    def __init__(self, img_size=400, patch_size=16, in_chans=3, embed_dim=768,
                 depth=12, num_heads=12, mlp_ratio=4.0, dropout=0.0, drop_path_rate=0.):
        super().__init__()
        self.patch_embed = PatchEmbed(img_size, patch_size, in_chans, embed_dim)
        num_patches = self.patch_embed.num_patches

        # Class token 與位置嵌入
        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))
        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))
        self.pos_drop = nn.Dropout(dropout)
        
        # 隨機深度衰減率
        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]
        self.blocks = nn.ModuleList([
            TransformerBlock(embed_dim, num_heads, mlp_ratio, dropout, dpr[i])
            for i in range(depth)
        ])
        self.norm = nn.LayerNorm(embed_dim)
        self._init_weights()
    
    def _init_weights(self):
        nn.init.trunc_normal_(self.pos_embed, std=0.02)
        nn.init.trunc_normal_(self.cls_token, std=0.02)
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.trunc_normal_(m.weight, std=0.02)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode="fan_out")
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
    
    def forward(self, x, interpolate_pos_encoding=False):
        # x: [B, 3, H, W]
        B = x.shape[0]
        x = self.patch_embed(x)  # [B, num_patches, embed_dim]
        cls_tokens = self.cls_token.expand(B, -1, -1)  # [B, 1, embed_dim]
        x = torch.cat((cls_tokens, x), dim=1)  # [B, num_patches+1, embed_dim]
        
        if interpolate_pos_encoding:
            N = x.shape[1]
            if N != self.pos_embed.shape[1]:
                cls_pos_embed = self.pos_embed[:, 0:1, :]
                patch_pos_embed = self.pos_embed[:, 1:, :]
                dim = x.shape[-1]
                num_patches = int(math.sqrt(N-1))
                orig_size = int(math.sqrt(self.pos_embed.shape[1]-1))
                patch_pos_embed = patch_pos_embed.reshape(1, orig_size, orig_size, dim).permute(0, 3, 1, 2)
                patch_pos_embed = F.interpolate(patch_pos_embed, size=(num_patches, num_patches), mode='bicubic', align_corners=False)
                patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).reshape(1, num_patches*num_patches, dim)
                x = torch.cat((cls_pos_embed, patch_pos_embed), dim=1)
            else:
                x = x + self.pos_embed
        else:
            x = x + self.pos_embed
        
        x = self.pos_drop(x)
        hidden_states = []
        for blk in self.blocks:
            x = blk(x)
            hidden_states.append(x)
        x = self.norm(x)
        return x, hidden_states

#########################################
# TransUNet: 結合完整 ViT 與 UNet-style Decoder
#########################################
class TransUNet(nn.Module):
    def __init__(self, num_decoder_conv_layers=5):
        """
        num_decoder_conv_layers:
         decoder 中要堆疊的卷積層數量（不含上採樣與最後 1x1 卷積）。
         此參數可以從 configs/default.yaml 調整，例如設為 30。
        """
        super().__init__()
        # 建立完整的 ViT 模型 (輸入尺寸固定為 400)
        self.vit = VisionTransformer(img_size=400, patch_size=16, in_chans=3,
                                     embed_dim=768, depth=12, num_heads=12,
                                     mlp_ratio=4.0, dropout=0.1, drop_path_rate=0.1)
        # 選擇一層作為 skip connection (例如第 6 層, index 5)
        self.skip_layer_index = 5
        
        # 建立深層 decoder，輸入通道數固定為 768*2 = 1536 (skip + final token)
        # 修改 decoder 結構使得輸出解析度能從 25x25 上採樣至 400x400，
        # 需要 4 次上採樣，故將 decoder 分為 5 個 block
        self.decoder = self.build_deep_decoder(num_layers=num_decoder_conv_layers)
    
    def build_deep_decoder(self, num_layers):
        """
        動態構造 decoder:
         - 將 decoder 分成 5 個 block，每個 block 中均勻分配卷積層數。
         - 每個 block 完成後（除最後一個 block 外）加入上採樣層 (scale_factor=2)。
         - 最後加入一個 1x1 卷積輸出分割結果。
        """
        num_blocks = 5  # 固定 block 數，這樣上採樣次數 = 4, 輸出解析度 25*(2^4)=400
        convs_per_block = num_layers // num_blocks
        remainder = num_layers % num_blocks  # 前 remainder 個 block 多一層
        
        layers = []
        in_channels = 1536  # 輸入通道數
        # 為每個 block 設計不同的輸出通道，參考原有結構： [512, 256, 128, 64, 32]
        channel_scheme = [512, 256, 128, 64, 32]
        for i in range(num_blocks):
            out_channels = channel_scheme[i]
            # 決定本 block 卷積層數
            layers_in_block = convs_per_block + (1 if i < remainder else 0)
            for j in range(layers_in_block):
                if j == 0:
                    layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))
                else:
                    layers.append(nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1))
                layers.append(nn.BatchNorm2d(out_channels))
                layers.append(nn.ReLU(inplace=True))
            # 除了最後一個 block，每個 block後加入上採樣
            if i < num_blocks - 1:
                layers.append(nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True))
            in_channels = out_channels
        # 最後一層: 1x1 卷積輸出單通道結果
        layers.append(nn.Conv2d(in_channels, 1, kernel_size=1))
        return nn.Sequential(*layers)
    
    def forward(self, x):
        B = x.shape[0]
        # 透過 ViT 得到最終 token 與各層隱藏狀態
        vit_out, hidden_states = self.vit(x, interpolate_pos_encoding=True)  # vit_out: [B, 1+N, 768]
        # 移除 class token 得到 token 序列
        final_tokens = vit_out[:, 1:, :]  # [B, N, 768]
        # 取得 skip connection token (來自第 skip_layer_index 層)
        skip_tokens = hidden_states[self.skip_layer_index][:, 1:, :]  # [B, N, 768]
        
        # 假設 token 數量 N 可排列成正方形，轉換成 2D feature maps
        N = final_tokens.shape[1]
        H = W = int(math.sqrt(N))
        final_feat = final_tokens.transpose(1, 2).contiguous().view(B, 768, H, W)
        skip_feat = skip_tokens.transpose(1, 2).contiguous().view(B, 768, H, W)
        
        # 串接 skip 與 encoder 最終特徵: [B, 1536, H, W]，H 與 W 應為 25
        feats = torch.cat([skip_feat, final_feat], dim=1)
        out = self.decoder(feats)
        return out

# 測試用
if __name__ == '__main__':
    # 可在 configs/default.yaml 中設定 num_decoder_conv_layers (如 30)
    model = TransUNet(num_decoder_conv_layers=30)
    dummy_input = torch.randn(1, 3, 400, 400)
    output = model(dummy_input)
    print("模型輸出 shape:", output.shape)



# ==== src/losses.py ====
import torch
import torch.nn as nn
import torch.nn.functional as F
import cv2
import numpy as np

def boundary_loss(pred, target):
    """
    利用距離變換計算邊界損失：
      - 將 target 0/1化
      - 計算 (1 - target) 的距離變換
      - 以距離作為權重計算 L1 損失
    """
    # 使用 sigmoid + clamp 防止極端數據發生
    pred = torch.sigmoid(pred).clamp(min=1e-7, max=1-1e-7)
    target = (target > 0.5).float()
    
    loss = 0.0
    B = target.shape[0]
    for i in range(B):
        target_np = target[i, 0].detach().cpu().numpy().astype(np.uint8)
        # 計算距離變化，限制最大值防止過大權重
        dist_map = cv2.distanceTransform(1 - target_np, cv2.DIST_L2, 5)
        dist_tensor = torch.tensor(dist_map, dtype=pred.dtype, device=pred.device)
        dist_tensor = torch.clamp(dist_tensor, max=10.0)
        loss += torch.mean(dist_tensor * torch.abs(pred[i, 0] - target[i, 0]))
    return loss / max(B, 1)

class DiceBCELoss(nn.Module):
    """
    混和 Dice Loss 和 BCE Loss。
    """
    def __init__(self, smooth=1e-5):
        super(DiceBCELoss, self).__init__()
        self.bce = nn.BCEWithLogitsLoss()
        self.smooth = smooth

    def forward(self, inputs, targets):
        # 使用 sigmoid + clamp 確保數值不要暴衝
        probs = torch.sigmoid(inputs).clamp(min=1e-7, max=1-1e-7)
        inputs_flat = probs.view(-1)
        targets_flat = targets.view(-1)
        intersection = (inputs_flat * targets_flat).sum()
        union = inputs_flat.sum() + targets_flat.sum()
        if union < self.smooth:
            return self.bce(inputs, targets)
        dice = (2. * intersection + self.smooth) / (union + self.smooth)
        dice_loss = 1 - dice
        bce_loss = self.bce(inputs, targets)
        return 0.5 * dice_loss + 0.5 * bce_loss

class FocalLoss(nn.Module):
    """
    Focal Loss 用于二元分割，加入 clamp 防止极端数值。
    """
    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean'):
        super(FocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction
    
    def forward(self, inputs, targets):
        inputs = torch.clamp(inputs, min=-88, max=88)
        BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')
        pt = torch.exp(-BCE_loss).clamp(min=1e-7, max=1-1e-7)
        gamma_factor = torch.clamp((1 - pt) ** self.gamma, max=100.0)
        focal_loss = self.alpha * gamma_factor * BCE_loss
        
        if self.reduction == 'mean':
            return focal_loss.mean()
        elif self.reduction == 'sum':
            return focal_loss.sum()
        else:
            return focal_loss

class DynamicDiceBCELoss(nn.Module):
    """
    動態調整 Dice+BCE 損失中 BCE 占比：
      - 初始 Dice 與 BCE 各50%
      - 從 stable_epoch 開始，BCE 的比重逐步上升到 max_bce_weight，
        而 Dice 下降，使總權重還是 100%
    """
    def __init__(self, smooth=1e-5, initial_bce_weight=0.5, max_bce_weight=1.0, stable_epoch=50, schedule_epochs=50):
        super(DynamicDiceBCELoss, self).__init__()
        self.bce = nn.BCEWithLogitsLoss()
        self.smooth = smooth
        self.initial_bce_weight = initial_bce_weight
        self.max_bce_weight = max_bce_weight
        self.stable_epoch = stable_epoch
        self.schedule_epochs = schedule_epochs

    def forward(self, inputs, targets, epoch):
        probs = torch.sigmoid(inputs).clamp(min=1e-7, max=1-1e-7)
        inputs_flat = probs.view(-1)
        targets_flat = targets.view(-1)
        intersection = (inputs_flat * targets_flat).sum()
        union = inputs_flat.sum() + targets_flat.sum()
        if union < self.smooth:
            dice_loss = self.bce(inputs, targets)
        else:
            dice = (2. * intersection + self.smooth) / (union + self.smooth)
            dice_loss = 1 - dice
        bce_loss = self.bce(inputs, targets)
        
        if epoch < self.stable_epoch:
            bce_weight = self.initial_bce_weight
        else:
            factor = min(1.0, (epoch - self.stable_epoch) / self.schedule_epochs)
            bce_weight = self.initial_bce_weight + (self.max_bce_weight - self.initial_bce_weight) * factor
        dice_weight = 1.0 - bce_weight
        
        return bce_weight * bce_loss + dice_weight * dice_loss

class CombinedLoss(nn.Module):
    """
    綜合損失函數：
      - 初期使用 DynamicDiceBCELoss（Dice+BCE ），
      - forward() 需要傳入當前 epoch
      - 返回一個元组 (總損失, dice_bce_loss, focal_loss, tversky_loss, boundary_loss)
      其他損失暫時返回 0.
    """
    def __init__(self, **kwargs):
        super(CombinedLoss, self).__init__()
        self.dynamic_dice_bce = DynamicDiceBCELoss(**kwargs)
    
    def forward(self, inputs, targets, epoch):
        loss = self.dynamic_dice_bce(inputs, targets, epoch)
        # 此版本只使用 DynamicDiceBCELoss，其他部分返回 0.
        return loss, loss, torch.tensor(0.0, device=inputs.device), torch.tensor(0.0, device=inputs.device), torch.tensor(0.0, device=inputs.device)


# ==== src/model_with_timm.py ====
import math
import torch
import torch.nn as nn
import torch.nn.functional as F
from timm import create_model

# 從原有模型匯入 TransUNet 結構（保留 decoder 與 skip connection）
from model import TransUNet

class TimMEncoderWrapper(nn.Module):
    def __init__(self, timm_model):
        super().__init__()
        self.timm_model = timm_model
        self.patch_embed = timm_model.patch_embed
        self.cls_token = timm_model.cls_token
        self.pos_embed = timm_model.pos_embed
        self.pos_drop = timm_model.pos_drop
        self.blocks = timm_model.blocks
        self.norm = timm_model.norm

    def interpolate_pos_encoding(self, x, w, h):
        n_patches = x.shape[1] - 1  # 排除 class token
        N = self.pos_embed.shape[1] - 1
        if n_patches == N and w == h:
            return self.pos_embed

        class_pos_embed = self.pos_embed[:, 0:1, :]
        patch_pos_embed = self.pos_embed[:, 1:, :]
        dim = x.shape[-1]
        orig_size = int(math.sqrt(N))

        patch_size = self.patch_embed.patch_size
        if isinstance(patch_size, int):
            patch_size = (patch_size, patch_size)
        new_grid_h = h // patch_size[0]
        new_grid_w = w // patch_size[1]
        patch_pos_embed = patch_pos_embed.reshape(1, orig_size, orig_size, dim).permute(0, 3, 1, 2)
        patch_pos_embed = F.interpolate(patch_pos_embed, size=(new_grid_h, new_grid_w),
                                        mode='bicubic', align_corners=False)
        patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).reshape(1, -1, dim)
        return torch.cat((class_pos_embed, patch_pos_embed), dim=1)

    def forward(self, x):
        B, C, H, W = x.shape
        x = self.patch_embed(x)
        cls_tokens = self.cls_token.expand(B, -1, -1)
        x = torch.cat((cls_tokens, x), dim=1)
        x = x + self.interpolate_pos_encoding(x, W, H)
        x = self.pos_drop(x)

        hidden_states = []
        for blk in self.blocks:
            x = blk(x)
            hidden_states.append(x)
        x = self.norm(x)
        return x, hidden_states

class TransUNetWithTimm(TransUNet):
    """
    利用 timm 預訓練 ViT（例如 vit_base_patch16_224_in21k）作 encoder，
    保持原有 TransUNet decoder 與 skip connection 結構，
    同時支援不同尺寸的輸入/輸出。
    """
    def __init__(self, img_size=400):
        super().__init__()
        # 建立 timm 預訓練模型，並更新模型內部尺寸參數以支援非 224 尺寸輸入
        timm_vit = create_model('vit_base_patch16_224_in21k', pretrained=True)
        # 將預設的 img_size 參數更新為指定尺寸（如 400）
        timm_vit.patch_embed.img_size = (img_size, img_size)
        if 'img_size' in timm_vit.default_cfg:
            timm_vit.default_cfg['img_size'] = img_size
        self.vit = TimMEncoderWrapper(timm_vit)

    def forward(self, x):
        B = x.shape[0]
        vit_out, hidden_states = self.vit(x)
        final_tokens = vit_out[:, 1:, :]
        skip_tokens = hidden_states[self.skip_layer_index][:, 1:, :]

        N = final_tokens.shape[1]
        H = W = int(math.sqrt(N))
        final_feat = final_tokens.transpose(1, 2).contiguous().view(B, 768, H, W)
        skip_feat = skip_tokens.transpose(1, 2).contiguous().view(B, 768, H, W)

        feats = torch.cat([skip_feat, final_feat], dim=1)
        out = self.decoder(feats)
        return out

if __name__ == '__main__':
    model = TransUNetWithTimm(img_size=400)
    dummy_input = torch.randn(1, 3, 400, 400)
    output = model(dummy_input)
    print("模型輸出 shape:", output.shape)

# ==== src/losses.py ====
import torch
import torch.nn as nn
import torch.nn.functional as F
import cv2
import numpy as np

def boundary_loss(pred, target):
    """
    利用距離變換計算邊界損失：
      - 將 target 0/1化
      - 計算 (1 - target) 的距離變換
      - 以距離作為權重計算 L1 損失
    """
    # 使用 sigmoid + clamp 防止極端數據發生
    pred = torch.sigmoid(pred).clamp(min=1e-7, max=1-1e-7)
    target = (target > 0.5).float()
    
    loss = 0.0
    B = target.shape[0]
    for i in range(B):
        target_np = target[i, 0].detach().cpu().numpy().astype(np.uint8)
        # 計算距離變化，限制最大值防止過大權重
        dist_map = cv2.distanceTransform(1 - target_np, cv2.DIST_L2, 5)
        dist_tensor = torch.tensor(dist_map, dtype=pred.dtype, device=pred.device)
        dist_tensor = torch.clamp(dist_tensor, max=10.0)
        loss += torch.mean(dist_tensor * torch.abs(pred[i, 0] - target[i, 0]))
    return loss / max(B, 1)

class DiceBCELoss(nn.Module):
    """
    混和 Dice Loss 和 BCE Loss。
    """
    def __init__(self, smooth=1e-5):
        super(DiceBCELoss, self).__init__()
        self.bce = nn.BCEWithLogitsLoss()
        self.smooth = smooth

    def forward(self, inputs, targets):
        # 使用 sigmoid + clamp 確保數值不要暴衝
        probs = torch.sigmoid(inputs).clamp(min=1e-7, max=1-1e-7)
        inputs_flat = probs.view(-1)
        targets_flat = targets.view(-1)
        intersection = (inputs_flat * targets_flat).sum()
        union = inputs_flat.sum() + targets_flat.sum()
        if union < self.smooth:
            return self.bce(inputs, targets)
        dice = (2. * intersection + self.smooth) / (union + self.smooth)
        dice_loss = 1 - dice
        bce_loss = self.bce(inputs, targets)
        return 0.5 * dice_loss + 0.5 * bce_loss

class FocalLoss(nn.Module):
    """
    Focal Loss 用于二元分割，加入 clamp 防止极端数值。
    """
    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean'):
        super(FocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction
    
    def forward(self, inputs, targets):
        inputs = torch.clamp(inputs, min=-88, max=88)
        BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')
        pt = torch.exp(-BCE_loss).clamp(min=1e-7, max=1-1e-7)
        gamma_factor = torch.clamp((1 - pt) ** self.gamma, max=100.0)
        focal_loss = self.alpha * gamma_factor * BCE_loss
        
        if self.reduction == 'mean':
            return focal_loss.mean()
        elif self.reduction == 'sum':
            return focal_loss.sum()
        else:
            return focal_loss

class DynamicDiceBCELoss(nn.Module):
    """
    動態調整 Dice+BCE 損失中 BCE 占比：
      - 初始 Dice 與 BCE 各50%
      - 從 stable_epoch 開始，BCE 的比重逐步上升到 max_bce_weight，
        而 Dice 下降，使總權重還是 100%
    """
    def __init__(self, smooth=1e-5, initial_bce_weight=0.5, max_bce_weight=1.0, stable_epoch=50, schedule_epochs=50):
        super(DynamicDiceBCELoss, self).__init__()
        self.bce = nn.BCEWithLogitsLoss()
        self.smooth = smooth
        self.initial_bce_weight = initial_bce_weight
        self.max_bce_weight = max_bce_weight
        self.stable_epoch = stable_epoch
        self.schedule_epochs = schedule_epochs

    def forward(self, inputs, targets, epoch):
        probs = torch.sigmoid(inputs).clamp(min=1e-7, max=1-1e-7)
        inputs_flat = probs.view(-1)
        targets_flat = targets.view(-1)
        intersection = (inputs_flat * targets_flat).sum()
        union = inputs_flat.sum() + targets_flat.sum()
        if union < self.smooth:
            dice_loss = self.bce(inputs, targets)
        else:
            dice = (2. * intersection + self.smooth) / (union + self.smooth)
            dice_loss = 1 - dice
        bce_loss = self.bce(inputs, targets)
        
        if epoch < self.stable_epoch:
            bce_weight = self.initial_bce_weight
        else:
            factor = min(1.0, (epoch - self.stable_epoch) / self.schedule_epochs)
            bce_weight = self.initial_bce_weight + (self.max_bce_weight - self.initial_bce_weight) * factor
        dice_weight = 1.0 - bce_weight
        
        return bce_weight * bce_loss + dice_weight * dice_loss

class CombinedLoss(nn.Module):
    """
    綜合損失函數：
      - 初期使用 DynamicDiceBCELoss（Dice+BCE ），
      - forward() 需要傳入當前 epoch
      - 返回一個元组 (總損失, dice_bce_loss, focal_loss, tversky_loss, boundary_loss)
      其他損失暫時返回 0.
    """
    def __init__(self, **kwargs):
        super(CombinedLoss, self).__init__()
        self.dynamic_dice_bce = DynamicDiceBCELoss(**kwargs)
    
    def forward(self, inputs, targets, epoch):
        loss = self.dynamic_dice_bce(inputs, targets, epoch)
        # 此版本只使用 DynamicDiceBCELoss，其他部分返回 0.
        return loss, loss, torch.tensor(0.0, device=inputs.device), torch.tensor(0.0, device=inputs.device), torch.tensor(0.0, device=inputs.device)



# ==== src/train.py ====
import matplotlib
matplotlib.use('Agg')

import os
import random
import yaml
import numpy as np
import cv2
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from dataset import SegmentationDataset
from model import TransUNet
from model_with_timm import TransUNetWithTimm
from tqdm import tqdm
from torch.cuda.amp import GradScaler, autocast
from utils import visualize
from predict import SievePredictor
from losses import CombinedLoss
import matplotlib.pyplot as plt

os.environ["NO_ALBUMENTATIONS_UPDATE"] = "1" #防止警告一直跳出來有夠煩

def update_loss_curve(loss_history, save_path="outputs/predictions/loss_curve.png"):
    epochs = range(1, len(loss_history['dice_bce']) + 1)
    plt.figure(figsize=(10, 6))
    plt.plot(epochs, loss_history['dice_bce'], label="Dice+BCE")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.title("Loss Curve (Dice+BCE)")
    plt.legend()
    plt.grid(True)
    plt.savefig(save_path)
    plt.close()
    print(f"Loss curve updated: {save_path}")

def main(config_path='configs/default.yaml'):
    # 載入配置檔
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # 建立資料載入器
    train_dataset = SegmentationDataset("data", mode='train', patch_size=config['patch_size'])
    train_loader = DataLoader(
        train_dataset,
        batch_size=config['batch_size'],
        shuffle=True,
        num_workers=4,
        pin_memory=True
    )
    
    val_dataset = SegmentationDataset("data", mode='val', patch_size=config['patch_size'])
    val_loader = DataLoader(
        val_dataset,
        batch_size=config['batch_size'],
        shuffle=False,
        num_workers=0,
        pin_memory=True
    )
    # 初始化模型
    if config.get('model_type', 'TransUNet') == "TransUNetWithTimm":
        model = TransUNetWithTimm().to(device)
    else:
        model = TransUNet().to(device)
    

    
    # 載入預訓練模型（若有）
    pretrained_path = os.path.join("data", "pretrained model", "pretrained_model.pth")
    if os.path.exists(pretrained_path):
        try:
            state_dict = torch.load(pretrained_path, map_location=device)
            model.load_state_dict(state_dict)
            print(f"成功載入預訓練模型: {pretrained_path}")
        except Exception as e:
            print(f"載入預訓練模型失敗: {e}\n將從頭開始訓練。")
    else:
        print(f"在 {pretrained_path} 找不到預訓練模型，將從頭開始訓練。")
    
    optimizer = optim.AdamW(model.parameters(), lr=config['lr'], weight_decay=1e-5)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)
    
    # 使用動態 Dice+BCE 損失
    criterion = CombinedLoss(initial_bce_weight=0.5, max_bce_weight=1.0,
                              stable_epoch=50, schedule_epochs=50)
    
    scaler = GradScaler()
    
    best_val_loss = float('inf')
    #這邊
    #best_model_state = None
    #revert_count = 0  # 記錄連續回滾次數
    #max_revert = 1500  # 超過此次數則停止訓練
    
    os.makedirs("outputs/models", exist_ok=True)
    os.makedirs("outputs/predictions", exist_ok=True)
    os.makedirs("outputs", exist_ok=True)  # 儲存損失折線圖
    
    loss_history = {"dice_bce": []}
    
    for epoch in range(config['epochs']):
        model.train()
        epoch_loss = 0.0
        valid_batches = 0
        
        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}', leave=False)
        for images, masks in progress_bar:
            images = images.to(device)
            masks = masks.to(device)
            optimizer.zero_grad()
            try:
                with autocast():
                    outputs = model(images)
                    loss, dice_bce_val, _, _, _ = criterion(outputs, masks, epoch)
                if torch.isnan(loss).any() or torch.isinf(loss).any():
                    print(f"Warning: NaN/Inf detected at epoch {epoch+1}. Skipping batch.")
                    continue
                scaler.scale(loss).backward()
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                scaler.step(optimizer)
                scaler.update()
                
                epoch_loss += loss.item()
                valid_batches += 1
                progress_bar.set_postfix({'loss': loss.item()})
            except Exception as e:
                print(f"Error in training batch: {e}")
                continue
        
        if valid_batches > 0:
            avg_train_loss = epoch_loss / valid_batches
            print(f'Epoch {epoch+1} Training Loss: {avg_train_loss:.4f}')
        else:
            print(f'Epoch {epoch+1} No valid training batches.')
            continue
        
        # 更新loss history (僅記錄 Dice+BCE 損失)
        loss_history["dice_bce"].append(dice_bce_val.item())
        update_loss_curve(loss_history, save_path="outputs/loss_curve.png")
        
        # 決定是否進行驗證：前100個 epoch，每 5 次驗證；100次以後，每個 epoch驗證
        do_validation = False
        if epoch + 1 < 100:
            if (epoch + 1) % 5 == 0:
                do_validation = True
        else:
            do_validation = True
        
        if do_validation:
            model.eval()
            val_loss = 0.0
            valid_val_batches = 0
            with torch.no_grad():
                for images, masks in val_loader:
                    images = images.to(device)
                    masks = masks.to(device)
                    try:
                        with autocast():
                            outputs = model(images)
                            loss_val, _, _, _, _ = criterion(outputs, masks, epoch)
                        if not (torch.isnan(loss_val).any() or torch.isinf(loss_val).any()):
                            val_loss += loss_val.item()
                            valid_val_batches += 1
                    except Exception as e:
                        print(f"Error in validation batch: {e}")
                        continue
            
            if valid_val_batches > 0:
                avg_val_loss = val_loss / valid_val_batches
                print(f'Epoch {epoch+1} Validation Loss: {avg_val_loss:.4f}')
                scheduler.step(avg_val_loss)
                
                # 可視化驗證結果（選擇隨機一張驗證圖）
                try:
                    val_img_dir = os.path.join("data", "val", "images")
                    val_mask_dir = os.path.join("data", "val", "masks")
                    val_files = [f for f in os.listdir(val_img_dir) if f.lower().endswith(('.jpg', '.png'))]
                    if len(val_files) > 0:
                        img_file = random.choice(val_files)
                        img_path = os.path.join(val_img_dir, img_file)
                        mask_path = os.path.join(val_mask_dir, os.path.splitext(img_file)[0] + '.png')
                        orig_img = cv2.imread(img_path)
                        true_mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)
                        if orig_img is not None and true_mask is not None:
                            predictor = SievePredictor(model=model, patch_size=config['patch_size'],
                                                        threshold=0.4, pred_batch_size=16)
                            pred_mask = predictor.predict_single_return(img_path)
                            save_path = os.path.join("outputs/predictions", f"val_epoch{epoch+1}.png")
                            visualize(orig_img, true_mask, pred_mask, save_path)
                            print(f"Validation visualisation saved to: {save_path}")
                except Exception as e:
                    print(f"Error during visualisation: {e}")
                
                # 先把"若驗證損失改善則更新最佳模型，否則回滾"的功能關掉
                if avg_val_loss < best_val_loss:
                    best_val_loss = avg_val_loss
                    best_model_state = model.state_dict()
                    torch.save(best_model_state, f"outputs/models/best_model.pth")
                    print("更新最佳模型.")
                    #revert_count = 0  # 重置回滾計數
                else:
                    #revert_count += 1
                    #print(f"Validation 沒有改善。Revert count: {revert_count}/{max_revert}")
                    print("本次驗證未改善，保留先前最佳模型狀態。")
                    #if revert_count >= max_revert:
                    #    print("Early stopping：連續回滾次數已達上限。")
                    #    break
                    #if best_model_state is not None:
                    #    model.load_state_dict(best_model_state)
                    #    print("回滾至最佳模型狀態。")
            else:
                print(f'Epoch {epoch+1} No valid validation batches.')
            
            # 每次驗證後都儲存一次當前模型（供日後檢查）
            torch.save(model.state_dict(), f"outputs/models/sievenet_epoch{epoch+1}.pth")
    
if __name__ == '__main__':
    main()


# ==== src/predict.py ====
import cv2
import torch
import numpy as np
import os
from tqdm import tqdm
from model import TransUNet

class SievePredictor:
    def __init__(self, model_path=None, patch_size=400, threshold=0.4, pred_batch_size=16, model=None):
        """
        初始化預測器
        :param model_path: 模型權重路徑 (若不傳入 model，則使用此參數)
        :param patch_size: 輸入 patch 尺寸
        :param threshold: 二值化閾值 (0-1)
        :param pred_batch_size: 預測時每個批次的 patch 數量
        :param model: 可選，直接傳入模型 (例如訓練驗證時使用當前模型)
        """
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.patch_size = patch_size
        self.threshold = threshold
        self.pred_batch_size = pred_batch_size
        
        if model is not None:
            self.model = model
        else:
            self.model = TransUNet().to(self.device)
            if model_path is None:
                raise ValueError("必須提供 model_path 或 model 參數")
            try:
                state_dict = torch.load(model_path, map_location=self.device)
                self.model.load_state_dict(state_dict)
                self.model.eval()
            except Exception as e:
                raise RuntimeError(f"模型加載失敗: {str(e)}")
        self.model.eval()
        self.kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))
        self.min_area = 100

    def predict_single_return(self, image_path):
        """
        預測單張圖像，返回預測後的二值 mask
        """
        image = cv2.imread(image_path)
        if image is None:
            raise ValueError(f"無法讀取圖像: {image_path}")
        
        h, w = image.shape[:2]
        full_mask = np.zeros((h, w), dtype=np.float32)
        count_mask = np.zeros((h, w), dtype=np.float32)
        pad = self.patch_size // 2
        
        image_padded = cv2.copyMakeBorder(image, pad, pad, pad, pad, cv2.BORDER_CONSTANT, value=[0, 0, 0])
        padded_h, padded_w = image_padded.shape[:2]
        stride = self.patch_size // 2
        
        patch_coords = []
        patches = []
        for y in range(0, padded_h - self.patch_size + 1, stride):
            for x in range(0, padded_w - self.patch_size + 1, stride):
                patch = image_padded[y:y+self.patch_size, x:x+self.patch_size]
                patches.append(self._preprocess(patch))
                patch_coords.append((y - pad, x - pad))
        
        predictions = []
        total = len(patches)
        for i in tqdm(range(0, total, self.pred_batch_size), desc="Batch Predict"):
            batch = torch.cat(patches[i:i+self.pred_batch_size], dim=0).to(self.device)
            with torch.no_grad():
                logits = self.model(batch)
                probs = torch.sigmoid(logits).cpu().numpy()
            predictions.extend([probs[j] for j in range(probs.shape[0])])
        
        for i, (y_orig, x_orig) in enumerate(patch_coords):
            y1 = max(y_orig, 0)
            x1 = max(x_orig, 0)
            y2 = min(y_orig + self.patch_size, h)
            x2 = min(x_orig + self.patch_size, w)
            
            patch_pred = predictions[i][0, :, :]
            if y_orig < 0 or x_orig < 0 or (y_orig + self.patch_size) > h or (x_orig + self.patch_size) > w:
                valid_patch = cv2.resize(patch_pred, (x2 - x1, y2 - y1))
            else:
                valid_patch = patch_pred
            
            full_mask[y1:y2, x1:x2] += valid_patch
            count_mask[y1:y2, x1:x2] += 1
        
        count_mask[count_mask == 0] = 1
        avg_mask = full_mask / count_mask
        
        bin_mask = (avg_mask > self.threshold).astype(np.uint8) * 255
        bin_mask = self._final_postprocess(bin_mask)
        return bin_mask

    def predict_single(self, image_path, output_path):
        bin_mask = self.predict_single_return(image_path)
        cv2.imwrite(output_path, bin_mask)
        print(f"預測結果已保存至: {output_path}")

    def _preprocess(self, image):
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = image.astype(np.float32) / 255.0
        tensor = torch.from_numpy(image).permute(2, 0, 1).unsqueeze(0)
        return tensor

    def _final_postprocess(self, mask):
        mask = cv2.medianBlur(mask, 5)
        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        for cnt in contours:
            if cv2.contourArea(cnt) < self.min_area:
                cv2.drawContours(mask, [cnt], -1, 0, -1)
        return mask

    def predict_batch(self, input_dir, output_dir):
        os.makedirs(output_dir, exist_ok=True)
        valid_ext = ('.png', '.jpg', '.jpeg', '.bmp', '.tiff')
        file_list = [f for f in os.listdir(input_dir) if f.lower().endswith(valid_ext)]
        if not file_list:
            print(f"目錄 {input_dir} 中沒有支持的圖像文件")
            return
        for filename in file_list:
            input_path = os.path.join(input_dir, filename)
            output_path = os.path.join(output_dir, f"pred_{filename}")
            try:
                print(f"\n正在處理: {filename}")
                self.predict_single(input_path, output_path)
            except Exception as e:
                print(f"處理 {filename} 失敗: {str(e)}")

if __name__ == '__main__':
    try:
        predictor = SievePredictor(
            model_path='outputs/models/best_model.pth',  # 調整成你最佳模型的路徑
            patch_size=400,
            threshold=0.4,
            pred_batch_size=16
        )
        predictor.predict_batch(
            input_dir='data/test/images',
            output_dir='outputs/predictions'
        )
    except Exception as e:
        print(f"運行時錯誤: {str(e)}")


# ==== src/utils.py ====
import cv2
import numpy as np
from sklearn.metrics import precision_score, recall_score, f1_score

def visualize(image, true_mask, pred_mask, save_path=None):
    """
    直接將原圖、正解與預測結果以原始像素值直列堆疊，並以 PNG 格式保存
    備註：輸入圖像 image 由 cv2.imread 取得，為 BGR 格式；
         true_mask 與 pred_mask 為單通道灰階圖，先轉為 BGR 再堆疊
    """
    # 若 true_mask 與 pred_mask 為單通道，轉換為 3 通道方便顯示
    if len(true_mask.shape) == 2:
        true_mask_color = cv2.cvtColor(true_mask, cv2.COLOR_GRAY2BGR)
    else:
        true_mask_color = true_mask.copy()
    if len(pred_mask.shape) == 2:
        pred_mask_color = cv2.cvtColor(pred_mask, cv2.COLOR_GRAY2BGR)
    else:
        pred_mask_color = pred_mask.copy()
    
    # 將三張圖片（原圖、正解、預測）直列（垂直）堆疊
    combined = np.vstack((image, true_mask_color, pred_mask_color))
    
    # 保存時使用 cv2.imwrite，直接輸出原始像素數值的 PNG 檔
    if save_path:
        cv2.imwrite(save_path, combined)

def calculate_metrics(true_mask, pred_mask):
    y_true = true_mask.flatten() > 127
    y_pred = pred_mask.flatten() > 127
    return {
        'precision': precision_score(y_true, y_pred),
        'recall': recall_score(y_true, y_pred),
        'f1': f1_score(y_true, y_pred),
        'iou': np.sum(y_true & y_pred) / np.sum(y_true | y_pred)
    }


# ==== src/postprocess.py ====

"""
postprocess 模組

本模組提供分割後處理工具與改善策略，
包含形態學後處理、CRF 細化以及多尺度融合，
以改善分割細節及邊界精度。
"""

import cv2
import numpy as np

def morphology_postprocess(binary_mask, kernel_size=5, min_area=100):
    mask = cv2.medianBlur(binary_mask, kernel_size)
    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (kernel_size, kernel_size))
    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)
    
    contours, _ = cv2.findContours(mask.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    for cnt in contours:
        if cv2.contourArea(cnt) < min_area:
            cv2.drawContours(mask, [cnt], -1, 0, -1)
    return mask

def apply_crf(image, probability_mask, iterations=5):
    try:
        import pydensecrf.densecrf as dcrf
        from pydensecrf.utils import unary_from_softmax
    except ImportError:
        raise ImportError("請先安裝 pydensecrf: pip install pydensecrf")

    h, w = probability_mask.shape
    d = dcrf.DenseCRF2D(w, h, 2)
    probs = np.stack([1 - probability_mask, probability_mask], axis=0)
    unary = unary_from_softmax(probs)
    d.setUnaryEnergy(unary)
    d.addPairwiseGaussian(sxy=3, compat=3)
    d.addPairwiseBilateral(sxy=80, srgb=13, rgbim=image, compat=10)
    Q = d.inference(iterations)
    preds = np.array(Q).reshape((2, h, w))
    refined_mask = (preds[1] > preds[0]).astype(np.uint8) * 255
    return refined_mask

def multi_scale_fusion(predictor, image, scales=[0.8, 1.0, 1.2], threshold=0.4):
    h, w = image.shape[:2]
    fused_mask = np.zeros((h, w), dtype=np.float32)
    count = np.zeros((h, w), dtype=np.float32)
    
    for scale in scales:
        resized = cv2.resize(image, (int(w * scale), int(h * scale)))
        pred_mask = predictor.predict_single_return_from_array(resized)
        pred_mask = cv2.resize(pred_mask, (w, h), interpolation=cv2.INTER_LINEAR).astype(np.float32) / 255.0
        fused_mask += pred_mask
        count += 1.0
    
    fused_mask = fused_mask / count
    bin_mask = (fused_mask > threshold).astype(np.uint8) * 255
    return bin_mask

if __name__ == '__main__':
    test_mask = cv2.imread("outputs/predictions/test_mask.png", cv2.IMREAD_GRAYSCALE)
    if test_mask is not None:
        proc_mask = morphology_postprocess(test_mask, kernel_size=5, min_area=100)
        cv2.imwrite("outputs/predictions/test_mask_post.png", proc_mask)
        print("形態學後處理結果已保存。")
    else:
        print("請先產生 test_mask.png 後再測試後處理。")


# ==== requirements.txt ====
--extra-index-url https://download.pytorch.org/whl/cu121
torch==2.1.1+cu121
torchvision==0.16.1+cu121
numpy>=1.22.0
opencv-python>=4.6.0
albumentations>=1.4.18
tqdm>=4.65.0
scikit-learn>=1.2.0
transformers>=4.31.0
PyYAML>=6.0
matplotlib>=3.6.0
timm>=0.6.12
pydensecrf>=1.2.0
